{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取IOB格式和分块语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text ='''he PRP B-VP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    "'''\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=('NP')).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print (conll2000.chunked_sents('train.txt',chunk_types=['NP'])[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单评估和基准\n",
    "\n",
    "我们开始为琐碎的不创建任何 块的块分析器 cp 建立一个基准\n",
    "\n",
    "现在让我们尝试一个初级 的正则表达式分块器，查找以名词短语标记的特征字母(如 CD、DT 和 JJ)开头的标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt',chunk_types=['NP'])\n",
    "print (cp.evaluate(test_sents))\n",
    "grammar = \"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print (cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "#使用n-gram标注器对名词短语分块\n",
    "class ngramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        '''类初始化 \n",
    "        chunk.tree2conlltags 返回包含（word、tag、iob标记）的三元组列表。将树转换为Conll IOB标记格式\n",
    "        UnigramTagger(train_data) 将unigram标注器训练成一元分块器\n",
    "        BigramTagger(train_data) 将Bigram标注器训练成二元分块器\n",
    "        '''\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.tag.BigramTagger(train_data)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]#获取词性标记数列\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)#对数列进行分块\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]#获取块数据\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]#组合打包词、词性、块数据\n",
    "        return nltk.chunk.conlltags2tree(conlltags)#返回树结构\n",
    "\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "ngram_chunker = ngramChunker(train_sents)\n",
    "print(ngram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Confidence', 'NN', 'B-NP')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.chunk.tree2conlltags(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练基于分类器的分块器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  95.0%%\n",
      "    Precision:     85.9%%\n",
      "    Recall:        90.0%%\n",
      "    F-Measure:     87.9%%\n"
     ]
    }
   ],
   "source": [
    "def tags_since_dt(sentence,i):\n",
    "    '''描述自最近的限定词以来遇到的所有词性标记'''\n",
    "    tags = set()\n",
    "    for word,pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))\n",
    "\n",
    "def npchunk_features(sentence,i,history):\n",
    "    '''提取第i个词的词性标记、词、上一个词的词性标记、后一个词的词性标记'''\n",
    "    word,pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword,prevpos=\"<START>\",\"<START>\"\n",
    "    else:\n",
    "        prevword,prevpos = sentence[i-1]\n",
    "    if i == len(sentence)-1:\n",
    "        nextword,nextpos='<END>',\"<END>\"\n",
    "    else:\n",
    "        nextword,nextpos=sentence[i+1]\n",
    "    return {\"pos\":pos,\"word\":word,\"prevpos\":prevpos,\"nextpos\":nextpos,\n",
    "            \"prevpos+pos\":\"%s+%s\" % (prevpos,pos),\n",
    "            \"pos+nextpos\":\"%s+%s\" % (pos,nextpos),\n",
    "            \"tags-since-dt\":tags_since_dt(sentence,i)\n",
    "           }\n",
    "\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)#去除分块标记\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                '''对句子的每个词的位置i进行遍历，提取(词,词性)特征featureset 以及分块标记tag'''\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset,tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        '''对数据进行打标'''\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset=npchunk_features(sentence,i,history)\n",
    "            tag=self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence,history)\n",
    "    \n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        '''转化树结构为((词,词性),分块标记)元组结构'''\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "    def parse(self, sentence):\n",
    "        '''标记结构转换至树结构'''\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print (chunker.evaluate(test_sents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
