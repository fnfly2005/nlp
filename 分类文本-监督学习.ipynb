{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鉴定性别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "0.736\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.3 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.1 : 1.0\n",
      "             last_letter = 'f'              male : female =     14.0 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.3 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "from nltk.classify import apply_features\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier \n",
    "from nltk.classify import accuracy\n",
    "\n",
    "def gender_features(word):\n",
    "    '''特征提取器'''\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "names = ([(name,'male') for name in names.words('male.txt')] +\n",
    "        [(name,'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(names)\n",
    "\n",
    "train_set = apply_features(gender_features,names[500:])\n",
    "test_set = apply_features(gender_features,names[:500])\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set) #训练朴素贝叶斯分类器\n",
    "\n",
    "print (classifier.classify(gender_features('Neo'))) #获取单个姓名预测值\n",
    "print (accuracy(classifier, test_set)) #获取准确率\n",
    "print (classifier.show_most_informative_features(5)) #查看最有效的5个特征-似然比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选择正确的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "source": [
    "def gender_features2(word):\n",
    "    return {'last_letter1': word[-1:],\n",
    "           'last_letter2': word[-2:]}\n",
    "\n",
    "train_set2 = apply_features(gender_features2,names[500:])\n",
    "test_set2 = apply_features(gender_features2,names[:500])\n",
    "\n",
    "classifier2 = NaiveBayesClassifier.train(train_set2) #训练朴素贝叶斯分类器\n",
    "\n",
    "print (accuracy(classifier2, test_set2)) #获取准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "#电影评论正负面分类任务\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words.keys())[:2000]\n",
    "def document_features(document):\n",
    "    '''特征提取器，用于提取前2000个高频词汇'''\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set3, test_set3 = featuresets[100:], featuresets[:100]\n",
    "classifier3 = NaiveBayesClassifier.train(train_set3)\n",
    "print (accuracy(classifier3, test_set3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于后缀训练词性分类器\n",
    "from nltk.corpus import brown\n",
    "from nltk.classify.decisiontree import DecisionTreeClassifier\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] +=1\n",
    "    suffix_fdist[word[-2:]] +=1\n",
    "    suffix_fdist[word[-3:]] +=1\n",
    "common_suffixes = list(suffix_fdist.keys())[:100]\n",
    "\n",
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets2 = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "size = int(len(featuresets2) * 0.02)\n",
    "size2 = int(len(featuresets2) * 0.1)\n",
    "train_set4, test_set4 = featuresets2[size:size2], featuresets2[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952262555942317\n",
      "if endswith(the) == False: \n",
      "  if endswith(s) == False: \n",
      "    if endswith(,) == False: \n",
      "      if endswith(.) == False: return 'IN'\n",
      "      if endswith(.) == True: return '.'\n",
      "    if endswith(,) == True: return ','\n",
      "  if endswith(s) == True: \n",
      "    if endswith(was) == False: \n",
      "      if endswith(as) == False: return 'NNS'\n",
      "      if endswith(as) == True: return 'HVZ'\n",
      "    if endswith(was) == True: return 'BEDZ'\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier4 = DecisionTreeClassifier.train(train_set4)\n",
    "print(accuracy(classifier4, test_set4))\n",
    "print(classifier4.pseudocode(depth=4))#伪代码解释器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索上下文语境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ton', 'prev-word': 'The'}\n",
      "0.7603182496270512\n"
     ]
    }
   ],
   "source": [
    "def pos_features2(sentence,i):\n",
    "    '''提取句子中指定位置词的最后3位的后缀，以及它的前一个词'''\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "print(pos_features2(brown.sents()[0], 1))\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets3 = []\n",
    "\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word,tag) in enumerate(tagged_sent):\n",
    "        featuresets3.append((pos_features2(untagged_sent,i),tag))\n",
    "\n",
    "size1 = int(len(featuresets3) * 0.02)\n",
    "size3 = int(len(featuresets3) * 0.1)\n",
    "train_set5, test_set5 = featuresets3[size1:size3], featuresets3[:size1]\n",
    "classifier5 = NaiveBayesClassifier.train(train_set5)\n",
    "print (accuracy(classifier5, test_set5))#利用上下文特征提高了我们的词性标注器的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列分类\n",
    "\n",
    "一种序列分类器策略，称为连续分类或贪婪序列分类，是为第一个输入找到最有可能的 类标签，然后使用这个问题的答案帮助找到下一个输入的最佳的标签。这个过程可以不断重 复直到所有的输入都被贴上标签。\n",
    "\n",
    "另一种方案是为词性标记所有可能的序列打分，选择总得分最高的序列。隐马尔可夫模 型就采取这种方法。隐马尔可夫模型类似于连续分类器，它不光看输入也看已预测标记的历 史。然而，不是简单地找出一个给定的词的单个最好的标签，而是为标记产生一个概率分布。 然后将这些概率结合起来计算标记序列的概率得分，最高概率的标记序列会被选中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7765858208955224\n"
     ]
    }
   ],
   "source": [
    "def pos_features3(sentence, i, history):\n",
    "    '''特征提取'''\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features3(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features3(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size4 = int(len(tagged_sents) * 0.02)\n",
    "size5 = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size4:size5], tagged_sents[:size4]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print (tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.936026936026936\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in nltk.corpus.treebank_raw.sents():\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)\n",
    "\n",
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prevword': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
    "featuresets4 = [(punct_features(tokens, i), (i in boundaries))\n",
    "                for i in range(1, len(tokens)-1)\n",
    "                if tokens[i] in '.?!']\n",
    "size = int(len(featuresets4) * 0.1)\n",
    "train_set6, test_set6 = featuresets4[size:], featuresets4[:size]\n",
    "classifier6 = NaiveBayesClassifier.train(train_set6)\n",
    "print (accuracy(classifier6, test_set6))\n",
    "\n",
    "def segment_sentences(words):\n",
    "    '''基于分类的断句器'''\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in words:\n",
    "        if word in '.?!' and classifier.classify(words, i) == True:\n",
    "            sents.append(words[start: i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 识别对话行为类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715\n"
     ]
    }
   ],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "def dialogue_act_features(post):\n",
    "    '''词袋模型之特征提取器'''\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains(%s)' % word.lower()] = True\n",
    "    return features\n",
    "featuresets5 = [(dialogue_act_features(post.text), post.get('class'))\n",
    "                for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets5[size:], featuresets5[:size]\n",
    "classifier7 = NaiveBayesClassifier.train(train_set)\n",
    "print (accuracy(classifier7,test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 识别文字蕴含\n",
    "\n",
    "rte_classify.RTEFeatureExtractor RTE语料库的简单分类器。\n",
    "\n",
    "它计算文本和假设之间单词和命名实体的重叠，以及假设中是否有单词/命名实体未能在文本中出现，因为这是一个指标，说明假设比文本（即不受文本约束）更具信息性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fledgling', 'terrorism.', 'Shanghai', 'former', 'Organisation', 'meeting', 'Davudi', 'association', 'at', 'was', 'central', 'that', 'Asia', 'four', 'operation', 'fight', 'Parviz', 'Soviet', 'China', 'representing', 'Russia', 'republics', 'SCO', 'binds', 'Iran', 'together', 'Co'}\n",
      "{'member', 'SCO.', 'China'}\n",
      "set()\n",
      "{'China'}\n",
      "{'member'}\n"
     ]
    }
   ],
   "source": [
    "def rte_features(rtepair):\n",
    "    extractor = nltk.classify.rte_classify.RTEFeatureExtractor(rtepair)\n",
    "    features={}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features\n",
    "\n",
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "extractor = nltk.classify.rte_classify.RTEFeatureExtractor(rtepair)\n",
    "print(extractor.text_words)\n",
    "print(extractor.hyp_words)\n",
    "print (extractor.overlap('word'))\n",
    "print (extractor.overlap('ne'))\n",
    "print (extractor.hyp_extra('word'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
